# webscraper_cloudbypass_api.py
# ä½¿ç”¨ç©¿äº‘APIç»•è¿‡Cloudflareé˜²æŠ¤

import requests
import time
import random
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

class CloudBypassAPIScraper:
    def __init__(self, api_key: str = "103fa266894f44d5b38248128b1d656d"):
        self.api_key = api_key
        self.api_base_url = "https://api.cloudbypass.com"
        self.session = requests.Session()
        self.setup_session()
    
    def setup_session(self):
        """è®¾ç½®ä¼šè¯"""
        self.session.headers.update({
            'x-cb-apikey': self.api_key,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        print(f"âœ… ç©¿äº‘APIåˆå§‹åŒ–å®Œæˆï¼ŒAPI Key: {self.api_key[:8]}...")

    def get_html_content(self, target_url: str, verbose: bool = False) -> Optional[str]:
        """
        è·å–æŒ‡å®šURLçš„HTMLå†…å®¹ï¼ˆé€šè¿‡ç©¿äº‘APIç»•è¿‡Cloudflareï¼‰
        
        Args:
            target_url: ç›®æ ‡URL
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
            
        Returns:
            HTMLå†…å®¹å­—ç¬¦ä¸²ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            if verbose:
                print(f"ğŸŒ©ï¸ ä½¿ç”¨ç©¿äº‘APIè·å–: {target_url}")
            
            # ä»ç›®æ ‡URLæå–ä¸»æœºå
            parsed_url = urlparse(target_url)
            host = parsed_url.netloc
            path = parsed_url.path
            if parsed_url.query:
                path += "?" + parsed_url.query
            
            # æ„å»ºAPIè¯·æ±‚URL
            api_url = f"{self.api_base_url}{path}"
            
            headers = {
                'x-cb-apikey': self.api_key,
                'x-cb-host': host,
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
            }
            
            if verbose:
                print(f"ğŸ“¡ API URL: {api_url}")
                print(f"ğŸ  ç›®æ ‡ä¸»æœº: {host}")
            
            # éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(1, 3))
            
            response = self.session.get(api_url, headers=headers, timeout=30)
            
            if verbose:
                print(f"ğŸ“Š å“åº”çŠ¶æ€ç : {response.status_code}")
                print(f"ğŸ“ å“åº”å¤§å°: {len(response.text)} å­—ç¬¦")
            
            if response.status_code == 200:
                # æ£€æŸ¥å“åº”å†…å®¹
                if "Just a moment" in response.text:
                    if verbose:
                        print("âš ï¸ å“åº”ä»åŒ…å«CloudflareæŒ‘æˆ˜é¡µé¢")
                    return None
                elif len(response.text) > 1000:  # åˆç†çš„é¡µé¢å¤§å°
                    if verbose:
                        print("âœ… ç©¿äº‘APIè°ƒç”¨æˆåŠŸ")
                    return response.text
                else:
                    if verbose:
                        print("âš ï¸ å“åº”å†…å®¹å¯èƒ½ä¸å®Œæ•´")
                    return response.text
            else:
                if verbose:
                    print(f"âŒ ç©¿äº‘APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
                
        except Exception as e:
            if verbose:
                print(f"âŒ ç©¿äº‘APIè°ƒç”¨å¼‚å¸¸: {e}")
            return None

    def extract_ucl_courses(self, html_content: str, source_url: str, verbose: bool = False) -> List[Dict]:
        """
        ä»HTMLå†…å®¹ä¸­æå–UCLè¯¾ç¨‹ä¿¡æ¯
        
        Args:
            html_content: HTMLå†…å®¹
            source_url: æºURL
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
            
        Returns:
            è¯¾ç¨‹ä¿¡æ¯åˆ—è¡¨
        """
        if not html_content:
            return []
            
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            courses = []
            
            if verbose:
                print(f"ğŸ” å¼€å§‹æå–è¯¾ç¨‹ä¿¡æ¯...")
                print(f"ğŸ“„ HTMLæ ‡é¢˜: {soup.title.string if soup.title else 'Unknown'}")
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰æ•ˆ
            if "Just a moment" in html_content:
                if verbose:
                    print("âš ï¸ å†…å®¹ä»ç„¶æ˜¯CloudflareæŒ‘æˆ˜é¡µé¢")
                return []
            
            # å¤šç§æå–ç­–ç•¥
            selectors_to_try = [
                '.result-item',
                '.course-item', 
                '.programme-item',
                '.course-card',
                '.degree-item',
                '[data-course]',
                '.search-result',
                '.course-listing',
                '.program-item',
                '.course-details',
                '.degree-programme'
            ]
            
            # å°è¯•CSSé€‰æ‹©å™¨
            for selector in selectors_to_try:
                elements = soup.select(selector)
                if elements:
                    if verbose:
                        print(f"âœ… æ‰¾åˆ° {len(elements)} ä¸ªè¯¾ç¨‹å…ƒç´  (é€‰æ‹©å™¨: {selector})")
                    for elem in elements:
                        course_info = self._extract_course_from_element(elem, source_url)
                        if course_info:
                            courses.append(course_info)
                    if courses:
                        break
            
            # å¦‚æœé€‰æ‹©å™¨æ²¡æ‰¾åˆ°ï¼Œå°è¯•é“¾æ¥æå–
            if not courses:
                if verbose:
                    print("ğŸ” å°è¯•é€šè¿‡é“¾æ¥æå–è¯¾ç¨‹ä¿¡æ¯...")
                links = soup.find_all('a', href=True)
                course_keywords = ['master', 'msc', 'ma', 'course', 'programme', 'degree', 'taught', 'graduate']
                
                for link in links:
                    text = link.get_text(strip=True)
                    href = link.get('href', '')
                    
                    if (len(text) > 25 and len(text) < 200 and 
                        any(keyword in text.lower() for keyword in course_keywords) and
                        ('ucl.ac.uk' in href or href.startswith('/'))):
                        
                        courses.append({
                            'title': text,
                            'url': urljoin(source_url, href),
                            'description': '',
                            'source': 'link_extraction'
                        })
                        
                        if len(courses) >= 50:
                            break
            
            if verbose:
                print(f"ğŸ“‹ æ€»å…±æå–åˆ° {len(courses)} ä¸ªè¯¾ç¨‹")
            return courses
            
        except Exception as e:
            if verbose:
                print(f"âŒ æå–è¯¾ç¨‹ä¿¡æ¯å¤±è´¥: {e}")
            return []

    def _extract_course_from_element(self, element, base_url: str) -> Optional[Dict]:
        """ä»å•ä¸ªå…ƒç´ æå–è¯¾ç¨‹ä¿¡æ¯"""
        try:
            # æå–æ ‡é¢˜
            title_selectors = ['h1', 'h2', 'h3', 'h4', '.title', '.course-title', '.programme-title', 'a']
            title = ""
            
            for selector in title_selectors:
                title_elem = element.select_one(selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    if title and len(title) > 5 and len(title) < 300:
                        break
            
            if not title:
                title = element.get_text(strip=True)[:200]
            
            # æå–é“¾æ¥
            link_elem = element.find('a', href=True)
            url = urljoin(base_url, link_elem.get('href', '')) if link_elem else base_url
            
            # æå–æè¿°
            desc_selectors = ['.description', '.summary', '.excerpt', 'p']
            description = ""
            for selector in desc_selectors:
                desc_elem = element.select_one(selector)
                if desc_elem:
                    description = desc_elem.get_text(strip=True)
                    if description and len(description) > 10:
                        break
            
            if title and len(title) > 5:
                return {
                    'title': title,
                    'url': url,
                    'description': description[:300],
                    'source': 'element_extraction'
                }
        except Exception as e:
            return None
        
        return None

    def close(self):
        """æ¸…ç†èµ„æº"""
        if self.session:
            self.session.close()


# å…¨å±€å®ä¾‹
_default_scraper = None

def get_default_scraper() -> CloudBypassAPIScraper:
    """è·å–é»˜è®¤çš„ç©¿äº‘APIçˆ¬è™«å®ä¾‹"""
    global _default_scraper
    if _default_scraper is None:
        _default_scraper = CloudBypassAPIScraper()
    return _default_scraper

def get_html_ucl_cloudbypass(url: str, verbose: bool = False) -> str:
    """
    ä½¿ç”¨ç©¿äº‘APIè·å–UCLç½‘é¡µHTMLå†…å®¹
    
    Args:
        url: ç›®æ ‡URL
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        
    Returns:
        HTMLå†…å®¹å­—ç¬¦ä¸²
    """
    scraper = get_default_scraper()
    html_content = scraper.get_html_content(url, verbose=verbose)
    return html_content or ""

def get_ucl_courses_cloudbypass(url: str = None, verbose: bool = False) -> List[Dict]:
    """
    ä½¿ç”¨ç©¿äº‘APIè·å–UCLè¯¾ç¨‹ä¿¡æ¯
    
    Args:
        url: ç›®æ ‡URLï¼Œé»˜è®¤ä¸ºUCLç ”ç©¶ç”Ÿè¯¾ç¨‹é¡µé¢
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        
    Returns:
        è¯¾ç¨‹ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«titleã€urlã€descriptionã€sourceå­—æ®µ
    """
    if url is None:
        url = "https://www.ucl.ac.uk/prospective-students/graduate/taught-degrees"
    
    scraper = get_default_scraper()
    html_content = scraper.get_html_content(url, verbose=verbose)
    
    if html_content:
        courses = scraper.extract_ucl_courses(html_content, url, verbose=verbose)
        return courses
    else:
        if verbose:
            print("âŒ æ— æ³•è·å–HTMLå†…å®¹")
        return []

def cleanup_scraper():
    """æ¸…ç†å…¨å±€çˆ¬è™«å®ä¾‹"""
    global _default_scraper
    if _default_scraper:
        _default_scraper.close()
        _default_scraper = None


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ç©¿äº‘API UCLè¯¾ç¨‹ä¿¡æ¯è·å–å·¥å…·")
    print("ğŸ’¡ ä½¿ç”¨ä¸“ä¸šçš„ç©¿äº‘APIæœåŠ¡ç»•è¿‡Cloudflareé˜²æŠ¤")
    
    try:
        # æµ‹è¯•è·å–è¯¾ç¨‹ä¿¡æ¯
        courses = get_ucl_courses_cloudbypass(verbose=True)
        
        print(f"\nğŸ“Š ç»“æœ:")
        print(f"ğŸ“ˆ æ‰¾åˆ°è¯¾ç¨‹æ€»æ•°: {len(courses)}")
        
        if courses:
            print(f"\nğŸ“ UCLè¯¾ç¨‹åˆ—è¡¨ (å‰10ä¸ª):")
            for i, course in enumerate(courses[:10], 1):
                print(f"\n{i}. {course['title']}")
                print(f"   ğŸ”— URL: {course['url']}")
                if course['description']:
                    print(f"   ğŸ“ æè¿°: {course['description'][:100]}...")
                print(f"   ğŸ” æ¥æº: {course['source']}")
        else:
            print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•è¯¾ç¨‹ä¿¡æ¯")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        cleanup_scraper() 